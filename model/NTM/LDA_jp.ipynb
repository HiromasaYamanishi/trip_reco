{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB8E9rEFHvuA"
      },
      "source": [
        "# Gensim LDA model for Japanese articles\n",
        "auther : m3yrin\n",
        "\n",
        "reference : http://tdual.hatenablog.com/entry/2018/04/09/133000\n",
        "\n",
        "### Memo\n",
        "* tdual' s LDA script is massively cited.\n",
        "* janome tokenizer is used instead of Mecab.\n",
        "\n",
        "### Dataset\n",
        "livedoor ニュースコーパス / livedoor News Corpus  \n",
        "https://www.rondhuit.com/download.html#ldcc  \n",
        "CC BY-ND 2.1 JP  \n",
        "https://creativecommons.org/licenses/by-nd/2.1/jp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ff3DsMsPH973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "Installing collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install janome\n",
        "\n",
        "import os\n",
        "if not os.path.exists('text'):\n",
        "    !wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "    !tar xvzf ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PV3arU_oHyWS"
      },
      "outputs": [],
      "source": [
        "from urllib import request \n",
        "import logging\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import re\n",
        "import janome\n",
        "import random\n",
        "from gensim import corpora, models\n",
        "\n",
        "from janome.tokenizer import Tokenizer\n",
        "from janome import analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WN90L6unXJqV"
      },
      "outputs": [],
      "source": [
        "# https://ohke.hateblo.jp/entry/2017/11/02/230000\n",
        "class NumericReplaceFilter(TokenFilter):\n",
        "    def apply(self, tokens):\n",
        "        for token in tokens:\n",
        "            parts = token.part_of_speech.split(',')\n",
        "            if (parts[0] == '名詞' and parts[1] == '数'):\n",
        "                token.surface = '0'\n",
        "                token.base_form = '0'\n",
        "                token.reading = 'ゼロ'\n",
        "                token.phonetic = 'ゼロ'\n",
        "            yield token\n",
        "\n",
        "            \n",
        "class docTokenizer:\n",
        "    def __init__(self, stopwords, parser=None, include_pos=None, exclude_posdetail=None, exclude_reg=None):\n",
        "    \n",
        "        self.stopwords = stopwords\n",
        "        self.include_pos = include_pos if include_pos else  [\"名詞\", \"動詞\", \"形容詞\"]\n",
        "        self.exclude_posdetail = exclude_posdetail if exclude_posdetail else [\"接尾\", \"数\"]\n",
        "        self.exclude_reg = exclude_reg if exclude_reg else r\"$^\"  # no matching reg\n",
        "        \n",
        "        self.char_filters = [\n",
        "                        UnicodeNormalizeCharFilter(), \n",
        "                        RegexReplaceCharFilter(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", u''), #url\n",
        "                        RegexReplaceCharFilter(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\", u''), #*.jp\n",
        "                        RegexReplaceCharFilter(self.exclude_reg, u'')\n",
        "                       ]\n",
        "        \n",
        "        self.token_filters = [\n",
        "                         NumericReplaceFilter(),\n",
        "                         POSKeepFilter(self.include_pos),\n",
        "                         POSStopFilter(self.exclude_posdetail), \n",
        "                         LowerCaseFilter()\n",
        "                        ]\n",
        "        \n",
        "        self.analyzer = analyzer.Analyzer(char_filters=self.char_filters, tokenizer=Tokenizer(), token_filters=self.token_filters)\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "\n",
        "        tokens = self.analyzer.analyze(text)\n",
        "        tokens = [re.sub(r\",\" ,\"\\t\", str(i)) for i in tokens]\n",
        "        l = [line.split(\"\\t\") for line in tokens]\n",
        "        \n",
        "        #Janome response\n",
        "        #i[] : ['認め', '動詞', '自立', '*', '*', '一段', '連用形', '認める', 'ミトメ', 'ミトメ']\n",
        "\n",
        "        res = []\n",
        "        for i in l:\n",
        "            if i[7] not in self.stopwords:\n",
        "                res.append(i[7])\n",
        "                \n",
        "        return res\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMR9WlVcNNkN"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gihn9x8xM08r"
      },
      "outputs": [],
      "source": [
        "num_articles = -1\n",
        "topic_num = 20\n",
        "passes = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tGTr_mNUB3"
      },
      "source": [
        "### Load stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i2fTDltSMBv4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Stopword :  928\n"
          ]
        }
      ],
      "source": [
        "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
        "stopwords = [line.decode(\"utf-8\").strip() for line in res]\n",
        "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/English.txt\")\n",
        "stopwords += [line.decode(\"utf-8\").strip() for line in res]\n",
        "\n",
        "stopwords += ['*', '&', '[', ']', ')', '(', '-',':','.','/','0', '...?', '——', '!【', '\"', ')、', ')。', ')」']\n",
        "\n",
        "print(\"# Stopword : \", len(stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDOwQry_NZQA"
      },
      "source": [
        "### Load articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e0TGFDNLXTk_"
      },
      "outputs": [],
      "source": [
        "doc_path = \"./text/\"\n",
        "doc_dir = Path(doc_path)\n",
        "dirs = [i for i in doc_dir.iterdir() if i.is_dir()]\n",
        "articles = [a for categ in dirs for a in categ.iterdir()]\n",
        "random.shuffle(articles)\n",
        "\n",
        "articles = articles[:num_articles]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yamanishi/.pyenv/versions/miniconda3-latest/envs/study_group/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1069153"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_exp = pd.read_csv('/home/yamanishi/project/trip_recommend/data/jalan/spot/experience_light.csv')\n",
        "df_review = pd.read_csv('/home/yamanishi/project/trip_recommend/data/jalan/review/review_all.csv')\n",
        "train_spot = df_exp[df_exp['valid']>=2].loc[:,'spot_name']\n",
        "valid_spot = df_exp[df_exp['valid']==1].loc[:, 'spot_name']\n",
        "test_spot = df_exp[df_exp['valid']==0].loc[:, 'spot_name']\n",
        "df_review_train = df_review[df_review['spot'].isin(train_spot)]\n",
        "len(df_review_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sPO_RtIPerWc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yamanishi/.pyenv/versions/miniconda3-latest/envs/study_group/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b39f6c111d6f418fbbc19b2bd7eaf49d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1069153 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = docTokenizer(stopwords = stopwords, exclude_reg=r\"\\d(年|月|日)\")\n",
        "\n",
        "docs = []\n",
        "for a in tqdm(df_review_train['review']):\n",
        "    docs.append(tokenizer.tokenize(a))\n",
        "    \"\"\"\n",
        "    with a.open() as f:\n",
        "        \n",
        "        # discard first two lines.\n",
        "        f.readline()\n",
        "        f.readline()\n",
        "        docs.append(tokenizer.tokenize(f.read()))\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6GpNDwPOjO_"
      },
      "source": [
        "### Build dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0qVrN67-5RH"
      },
      "outputs": [],
      "source": [
        "# build dict\n",
        "d = corpora.Dictionary(docs)\n",
        "d.filter_extremes(no_below=5, no_above=0.2)\n",
        "d.compactify()\n",
        "\n",
        "# make bow\n",
        "corpus = [d.doc2bow(w) for w in docs]\n",
        "\n",
        "# test train split\n",
        "test_size = int(len(corpus) * 0.1)\n",
        "test_corpus = corpus[:test_size]\n",
        "train_corpus = corpus[test_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79l99--OqHk"
      },
      "source": [
        "### Build LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2PkzvVa_GCG"
      },
      "outputs": [],
      "source": [
        "# logging setting\n",
        "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
        "\n",
        "# build LDA\n",
        "lda = models.LdaModel(corpus=train_corpus, id2word=d,num_topics=topic_num,passes=10, update_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUBDZv8xO1dt"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWB2L_WQOWI4"
      },
      "outputs": [],
      "source": [
        "N = sum(count for doc in train_corpus for id, count in doc)\n",
        "print(\"# of words in train corpas: \",N)\n",
        "perplexity = np.exp2(-lda.log_perplexity(train_corpus))\n",
        "print(\"perplexity(train):\", perplexity)\n",
        "\n",
        "print(\"==============================\")\n",
        "N = sum(count for doc in test_corpus for id, count in doc)\n",
        "print(\"# of words in test corpas: \",N)\n",
        "perplexity = np.exp2(-lda.log_perplexity(test_corpus))\n",
        "print(\"perplexity(test):\", perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqGs1fKK_rgq"
      },
      "outputs": [],
      "source": [
        "def get_topic_words(topic_id):\n",
        "    tw = []\n",
        "    for t in lda.get_topic_terms(topic_id):\n",
        "        tw.append(d[t[0]])\n",
        "    \n",
        "    return tw\n",
        "\n",
        "for t in range(topic_num):\n",
        "    tw = get_topic_words(t)\n",
        "    print('Topic {}: {}'.format(t + 1, ' '.join(tw)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LDA_jp.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 64-bit ('study_group')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7a865c41608e708ace9ed6e40a66ff8f45d4fe73a2fce76e3bb9141e87d20f44"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
